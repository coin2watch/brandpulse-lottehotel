from flask import Flask
from playwright.sync_api import sync_playwright
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime, timedelta
import openai
import os
import json
import re
import threading

app = Flask(__name__)

def get_worksheet(log):
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds_json = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON")
    if not creds_json:
        log.append("âŒ GOOGLE_SERVICE_ACCOUNT_JSON í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        raise ValueError("GOOGLE_SERVICE_ACCOUNT_JSON í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    try:
        creds_dict = json.loads(creds_json)
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        spreadsheet = client.open("BrandPulse_Lotte_Hotel")
        worksheet = spreadsheet.worksheet("BlogData")
        log.append("âœ… êµ¬ê¸€ì‹œíŠ¸ ì¸ì¦ ë° ì ‘ê·¼ ì„±ê³µ")
        return worksheet
    except Exception as e:
        log.append(f"âŒ êµ¬ê¸€ì‹œíŠ¸ ì¸ì¦/ì ‘ê·¼ ì‹¤íŒ¨: {e}")
        raise

def analyze_summary(title, link, log):
    openai.api_key = os.environ.get("OPENAI_API_KEY")
    if not openai.api_key:
        log.append("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    prompt = f"ì•„ë˜ ë¸”ë¡œê·¸ ì œëª©ê³¼ ë§í¬ë¥¼ ì°¸ê³ í•´ì„œ, ë§ˆì¼€íŒ…/ì†Œë¹„ì ë°˜ì‘ ê´€ì ì—ì„œ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\n\nì œëª©: {title}\në§í¬: {link}"
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        summary = response.choices[0].message.content.strip()
        log.append(f"âœ… ìš”ì•½ ì„±ê³µ: '{title}' -> {summary[:30]}...")
        return summary
    except Exception as e:
        log.append(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ì‹¤íŒ¨"

def parse_post_date(raw_date):
    today = datetime.now()
    if "ì¼ ì „" in raw_date:
        days = int(re.search(r"(\d+)ì¼ ì „", raw_date).group(1))
        post_date = today - timedelta(days=days)
    elif "ì‹œê°„ ì „" in raw_date:
        post_date = today
    elif re.match(r"\d{4}\.\d{2}\.\d{2}\.$", raw_date):
        post_date = datetime.strptime(raw_date.rstrip("."), "%Y.%m.%d")
    elif re.match(r"\d{4}\.\d{2}\.\d{2}", raw_date):
        post_date = datetime.strptime(raw_date[:10], "%Y.%m.%d")
    else:
        post_date = today
    return post_date.strftime("%Y-%m-%d")

def is_this_week(post_date):
    today = datetime.now()
    start = today - timedelta(days=today.weekday())
    end = start + timedelta(days=6)
    post_dt = datetime.strptime(post_date, "%Y-%m-%d")
    return start.date() <= post_dt.date() <= end.date()

def get_existing_links(worksheet, log):
    try:
        links = worksheet.col_values(6)
        log.append(f"âœ… ê¸°ì¡´ ë§í¬ {len(links)}ê°œ ë¶ˆëŸ¬ì˜´")
    except Exception as e:
        links = []
        log.append(f"âŒ ê¸°ì¡´ ë§í¬ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    return set(links)

def crawl_naver_blogs(keywords, log):
    results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox"])
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={'width': 1280, 'height': 800},
            locale='ko-KR'
        )
        for keyword in keywords:
            page = context.new_page()
            url = f"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={keyword}"
            log.append(f"â–¶ï¸ í¬ë¡¤ë§ ì‹œì‘: {keyword}")
            try:
                page.goto(url, timeout=60000)
                page.wait_for_timeout(3000)
                items = page.query_selector_all("li.bx")
                log.append(f"ğŸ” {keyword} ê²€ìƒ‰ê²°ê³¼ {len(items)}ê±´")
                count = 0
                for item in items:
                    if count >= 5:
                        break
                    title_el = item.query_selector("a.title_link")
                    date_el = item.query_selector("span.sub")
                    if not title_el or not date_el:
                        log.append("â—ï¸ title ë˜ëŠ” date ìš”ì†Œ ëˆ„ë½")
                        continue
                    title = title_el.inner_text()
                    link = title_el.get_attribute("href")
                    raw_date = date_el.inner_text()
                    post_date = parse_post_date(raw_date)
                    if not is_this_week(post_date):
                        log.append(f"âŒ ì´ë²ˆì£¼ ì•„ë‹˜: {post_date}")
                        continue
                    summary = analyze_summary(title, link, log)
                    crawl_date = datetime.now().strftime("%Y-%m-%d")
                    results.append([crawl_date, post_date, keyword, title, summary, link])
                    log.append(f"âœ… ìˆ˜ì§‘: {title[:20]}... ({post_date})")
                    count += 1
                page.close()
            except Exception as e:
                log.append(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨ ({keyword}): {e}")
        browser.close()
    return results

def run_blog_crawler(log):
    try:
        worksheet = get_worksheet(log)
        keywords = ["ë¡¯ë°í˜¸í…”", "ì‹ ë¼í˜¸í…”", "ì¡°ì„ í˜¸í…”"]
        existing_links = get_existing_links(worksheet, log)
        data = crawl_naver_blogs(keywords, log)
        new_data = [row for row in data if row[-1] not in existing_links]
        log.append(f"í•„í„° í›„ ì‹ ê·œ ë°ì´í„°: {len(new_data)}ê±´")
        if new_data:
            try:
                worksheet.append_rows(new_data, value_input_option="USER_ENTERED")
                log.append(f"âœ… {len(new_data)}ê±´ì˜ ë°ì´í„°ê°€ êµ¬ê¸€ ì‹œíŠ¸ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                log.append(f"âŒ êµ¬ê¸€ì‹œíŠ¸ append_rows ì‹¤íŒ¨: {e}")
        else:
            log.append("âŒ ì €ì¥í•  ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        log.append(f"run_blog_crawler ì˜ˆì™¸: {e}")

@app.route("/")
def index():
    return "âœ… BrandPulse Blog Crawler Running"

@app.route("/run-sync")
def run_sync():
    log = []
    run_blog_crawler(log)
    return "<br>".join(log)

@app.route("/run")
def run():
    log = []
    def task():
        run_blog_crawler(log)
    t = threading.Thread(target=task)
    t.start()
    return "<br>".join(["ğŸ”¥ /run ë¼ìš°íŠ¸ í˜¸ì¶œë¨"] + log + ["(ì‹¤í–‰ ì¤‘, ìƒˆë¡œê³ ì¹¨í•˜ë©´ ìµœì‹  ë¡œê·¸ê°€ ê°±ì‹ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)"])

@app.route("/env-debug")
def env_debug():
    val = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON")
    if not val:
        return "GOOGLE_SERVICE_ACCOUNT_JSON is NOT set!", 500
    return f"Length: {len(val)}<br>Start: {val[:100]}<br>End: {val[-100:]}"

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port)
