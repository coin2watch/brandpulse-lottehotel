import os
import time
from datetime import datetime
from playwright.sync_api import sync_playwright
from utils import (
    logger, get_today, is_duplicate, extract_keywords_from_text,
    get_gsheet_client, summarize_with_gpt
)

# ğŸ‘‰ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜: êµ¬ê¸€ ì‹œíŠ¸ ID
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")

# ğŸ‘‰ ëŒ€ìƒ ë¸Œëœë“œ
BRANDS = ["ë¡¯ë°í˜¸í…”", "ì‹ ë¼í˜¸í…”", "ì¡°ì„ í˜¸í…”", "ë² ìŠ¤íŠ¸ì›¨ìŠ¤í„´"]

# âœ… ë¸”ë¡œê·¸ ê²€ìƒ‰ í•¨ìˆ˜
def search_naver_blog(brand, max_results=5):
    logger.info(f"[ë¸”ë¡œê·¸ ìˆ˜ì§‘] {brand}")
    blogs = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        query = f"{brand} ë¸”ë¡œê·¸"
        url = f"https://search.naver.com/search.naver?where=view&sm=tab_jum&query={query}"
        page.goto(url)
        page.wait_for_timeout(3000)

        items = page.query_selector_all("li.bx._svp_item")
        logger.info(f"[{brand}] ê²€ìƒ‰ëœ ë¸”ë¡œê·¸ ìˆ˜: {len(items)}")

        for item in items[:max_results]:
            try:
                title_el = item.query_selector("a.api_txt_lines.total_tit")
                title = title_el.inner_text()
                link = title_el.get_attribute("href")
                text_snippet_el = item.query_selector("div.api_txt_lines.dsc_txt")
                snippet = text_snippet_el.inner_text() if text_snippet_el else ""
                blogs.append({"title": title, "link": link, "snippet": snippet})
            except Exception as e:
                logger.warning(f"ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

        browser.close()
    return blogs

# âœ… ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜
def run():
    try:
        logger.info("âœ… [Blog] ì‹¤í–‰ ì‹œì‘")
        today = get_today()

        # ì‹œíŠ¸ ì—°ê²°
        sheet = get_gsheet_client(SPREADSHEET_ID, "BlogData")
        insight_sheet = get_gsheet_client(SPREADSHEET_ID, "BlogInsights")

        for brand in BRANDS:
            blogs = search_naver_blog(brand)
            for blog in blogs:
                title = blog["title"]
                link = blog["link"]
                snippet = blog["snippet"]

                if is_duplicate(sheet, today, brand, title):
                    continue

                keywords = extract_keywords_from_text(title + " " + snippet)
                sentiment_keywords = [k for k in keywords if any(s in k for s in ["ì¢‹", "ë§Œì¡±", "ì‹«", "ë¶ˆë§Œ"])]
                summary = summarize_with_gpt(title + " " + snippet)

                sheet.append_row(
                    [today, brand, title, ", ".join(sentiment_keywords), link],
                    value_input_option="USER_ENTERED"
                )
                insight_sheet.append_row(
                    [today, brand, title, ", ".join(keywords), summary, link],
                    value_input_option="USER_ENTERED"
                )
                time.sleep(1)

        logger.info("âœ… [Blog] ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ [Blog Error] {e}")
        import traceback
        traceback.print_exc()
        
